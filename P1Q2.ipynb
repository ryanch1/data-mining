{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "############# pre-setup\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "##############\n",
    "\n",
    "categories = ['comp.graphics', 'comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "train_dataset = fetch_20newsgroups(subset = 'train', categories = categories,\n",
    " shuffle = True, random_state = None)\n",
    "test_dataset = fetch_20newsgroups(subset = 'test', categories = categories,\n",
    " shuffle = True, random_state = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "From: sac@asdi.saic.com (Steve A. Conroy x6172)\n",
      "Subject: Re: Darrrrrrrrryl\n",
      "Organization: SAIC\n",
      "Lines: 33\n",
      "\n",
      "In article <mssC5KCru.5Ip@netcom.com>, mss@netcom.com (Mark Singer) writes:\n",
      "|> \n",
      "|> \n",
      "|> The media is beating the incident at Dodger Stadium on Wednesday to\n",
      "|> death, but I haven't seen anything in rsb yet.\n",
      "|> \n",
      "|> Gerald Perry of the Cardinals pinch hit in the eighth inning with two\n",
      "|> on and his club down by a run.  He stroked a line drive into the\n",
      "|> right field corner.  The ball cleared the three-foot high fence and\n",
      "|> went into the crowd.  Darryl, racing over from right center, got to\n",
      "|> the spot in time to reach his glove up over the short fence, but he\n",
      "|> missed the ball.  A fan sitting in the front row, wearing a mitt,\n",
      "|> reached up and caught the ball.  Home run.\n",
      "|> \n",
      "|> Now I've seen the replay several times and I have concluded that\n",
      "|> Darryl missed the ball, and that the fan's glove was essentially\n",
      "|> behind Darryl's.  Several Dodger fans with seats in the immediate\n",
      "|> vicinity have claimed that the fan unquestionably interfered with\n",
      "|> Strawberry.  What cannot be disputed, however, is that the fan\n",
      "|> who caught the ball never took his eye off it;  he was oblivious\n",
      "|> to where the fielder was playing.  He was also quite exuberant as\n",
      "|> soon as he realized he had made the catch.\n",
      "|> \n",
      "|> [Stuff about Daryl and Tommy and everyone blaming fan for the loss deleted]\n",
      "\n",
      "I saw the replay several times too.  No question about it.  Daryl missed\n",
      "the ball, *then* the fan caught it.  Daryl is so tall that he had the\n",
      "first shot at the ball.  Daryl's just whining again.  I think it shows a\n",
      "lack of class when Tommy, Daryl and the Dodgers blame a single fan for\n",
      "losing the game.  What about the pitcher who threw up the gopher ball?\n",
      "What about the pitchers that gave up 6 runs up to that point?  Sorry, Tommy.\n",
      "If it were a 2-1 game and Daryl was 5 feet 2 inches tall, then maybe -\n",
      "just maybe - you'd have an argument.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))\n",
    "print(train_dataset.keys())\n",
    "print(train_dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "                strip_accents=None, token_pattern='(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "(4732, 18754)\n",
      "(4732, 18754)\n",
      "--------------------\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "--------------------\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#exclude numbers using regex\n",
    "#r'\\b[^\\d\\W]+\\b gets rid of numbers but not weird text like '___' '_j'\n",
    "#r'(?ui)\\b\\w*[a-z]+\\w*\\b' gets rid of weird text but not all numbers\n",
    "tmp_vect = CountVectorizer(min_df=3, stop_words='english'\n",
    "                             ,token_pattern=r'(?ui)\\b\\w*[a-z]+\\w*\\b')\n",
    "print(tmp_vect)\n",
    "\n",
    "#transform dataset into a matrix using fit_transform.\n",
    "X_train_tmp = tmp_vect.fit_transform(train_dataset.data)\n",
    "print(X_train_tmp.shape)\n",
    "\n",
    "#check output\n",
    "#print(tmp_vect.get_feature_names())\n",
    "\n",
    "#tdif step\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_tmp)\n",
    "print(X_train_tfidf.shape)\n",
    "print('-' * 20)\n",
    "print(X_train_tmp.toarray()[:30,:5])\n",
    "print('-' * 20)\n",
    "print(X_train_tfidf.toarray()[:30,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lemmatization to occur later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
