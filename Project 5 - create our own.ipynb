{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install vaderSentiment","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n#note: depending on how you installed (e.g., using source code download versus pip install), you may need to import like this:\n#from vaderSentiment import SentimentIntensityAnalyzer\n\nimport itertools\nimport json\nimport string\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction import text, stop_words\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom string import punctuation\nfrom string import digits\nfrom nltk.corpus import wordnet\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix,roc_curve\nfrom sklearn.model_selection import train_test_split\nimport json\nimport re\nimport pandas as pd\n\n\ndef my_custom_preprocessor(doc_string):\n    # do all data preprocessing here\n    \n    # Lower case\n    doc_string=doc_string.lower()\n    \n    # Remove Numbers\n    remove_digits = str.maketrans('', '', digits)\n    doc_string.translate(remove_digits)\n    \n    # Convert to tokenized form....\n    tokens = nltk.tokenize.word_tokenize(doc_string)\n    # Iterate through list of tokens (words) and remove all numbers\n    tokens = [word for word in tokens if word.isalpha()]\n    # Iterate through list of tokens (words) and stem (shorten) each word\n    port_stemmer = PorterStemmer()\n    tokens = [port_stemmer.stem(words) for words in tokens ]\n    \n    ###############################\n    #### Lemmatize with pos_tag ###\n    ###############################\n    \n    lemmatizer = WordNetLemmatizer()\n    \n    # Convert between two different tagging schemes\n    def change_tags(penntag):\n        morphy_tag = {'NN':'n', 'JJ':'a',\n                      'VB':'v', 'RB':'r'}\n        try:\n            return morphy_tag[penntag[:2]]\n        except:\n            return 'n'\n        \n    tokens = [lemmatizer.lemmatize(word.lower(), pos=change_tags(tag)) for word, tag in pos_tag(tokens)]\n    \n    # Rejoin List of tokens and return that single document-string\n    return ' '.join(tokens)\n\n\n#####################################################\n#### Define Custom stop words for CountVectorizer ###\n#####################################################\n\nstop_words_skt = text.ENGLISH_STOP_WORDS\nstop_words_en = stopwords.words('english')\ncombined_stopwords = set.union(set(stop_words_en),set(punctuation),set(stop_words_skt))\n\n# Run stop_words through the same pre-processor as the document-matrix\n# This will apply stemmed/lemmatized stop_woirds to stemmed/lemmatized tokenized document lists\ndef process_stop_words(stop_word_set):\n    doc_string = ' '.join(stop_word_set)\n    return my_custom_preprocessor(doc_string).split()\n","execution_count":2,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n  warnings.warn(message, FutureWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_files = [r'../input/gohawkspats/tweets_gohawks.txt', r'../input/gohawkspats/tweets_gopatriots.txt']\n\ngo_pats = []\ngo_hawks = []\n\nanalyzer = SentimentIntensityAnalyzer()\n\n\nfor file in tweet_files:\n    with open(file, 'r') as cur_file:\n        for line in cur_file:\n            json_data = json.loads(line)\n            time = json_data['citation_date']\n            text = json_data['tweet']['text']\n            vader_sent = analyzer.polarity_scores(text)\n\n            if file == r'../input/gohawkspats/tweets_gohawks.txt':\n                go_hawks.append((text, time, vader_sent))\n            else:\n                go_pats.append((text, time, vader_sent))\n\ngo_hawks = np.array(go_hawks)\ngo_pats = np.array(go_pats)\nprint(go_hawks[:,0])\n","execution_count":3,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"list indices must be integers or slices, not tuple","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-30a52c6cdb05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mgo_pats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvader_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgo_hawks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\narr_hawks = np.array(go_hawks)\narr_pats = np.array(go_pats)\nprint(arr_hawks.shape)\nprint(arr_pats.shape)\nprint(arr_pats[:,1])\n#tweet, time, sentiment = 0, 1, 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#comment","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}